{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuraliKrishnadata/google_colab_working_files/blob/main/sarima_Assinment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuqvWCjM4zj1"
      },
      "source": [
        "# SARIMA forecast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "heading_collapsed": true,
        "id": "U3b_AhQD4zj2"
      },
      "source": [
        "## 01 - Background\n",
        "\n",
        "Our goal in this challenge is to apply the basic concepts of time series analysis on one-dimension data (sales depending on the date).\n",
        "\n",
        "In this challenge, we'll go through the following steps :\n",
        "1. load and visualize the data;\n",
        "2. train our models and make predictions;\n",
        "3. use an econometric approach to model the serie and be able to forecast it;\n",
        "4. use machine learning to hack this modelization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG3QiWJi4zj3"
      },
      "source": [
        "## 02 - Load Data\n",
        "Let's start by loading the time series of the challenge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp3RDBfz4zj3"
      },
      "outputs": [],
      "source": [
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/monthly_champagne_sales.csv\")"
      ],
      "metadata": {
        "id": "ymr_pIQV49-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sample(10)"
      ],
      "metadata": {
        "id": "BxjIkljV5Jqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HtdxN3u4zj3"
      },
      "source": [
        "üëâ Convert dataframe with datetime objects as index to make it look like this\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv1Av1iD4zj3"
      },
      "outputs": [],
      "source": [
        "data['Month'] = pd.to_datetime(data['Month'])\n",
        "data.set_index('Month', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.sample(10)"
      ],
      "metadata": {
        "id": "Ku-boYYF5bWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.rename(columns={'Month': 'Date'}, inplace=True)"
      ],
      "metadata": {
        "id": "WSq6Jky_5hUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "rZvJSW2C5wfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop('month', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "cfd2LTZD6D6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlWpevEw4zj4"
      },
      "source": [
        "## 04 - Visualize and interpet the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iguloma4zj4"
      },
      "source": [
        "Well done, thanks to this \"reindexing\", you should now be able to plot the \"Sales\" (y-axis) as a function of the time (x-axis) easily"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data.index, data['Sales'])\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Monthly Champagne Sales Over Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wTCnD6vI6AC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft9srlzC4zj4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrwqYbEB4zj4"
      },
      "source": [
        "If your code is correct, you should be able to see that this Time Serie (TS) is:\n",
        "- Not stationary (mean and variance are not constant).\n",
        "- Exhibits strong seasonality.\n",
        "- Seems to have a trend.\n",
        "\n",
        "Let's see a decomposition of the data between **trend**, **saisonality** and **noise**. In order to do that, you have to make use of statsmodels [`seasonal_decompose`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html). Read the docs and make sure you understand what this function is doing and how to use it. Then:\n",
        "1. plot the \"Sales\" with an \"additive\" model\n",
        "2. plot the \"Sales\" with a \"multiplicative\" model\n",
        "\n",
        "Pro tips: end your statsmodels plot method calls with a `;` to avoid double-plotting issues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuTj_xJg4zj4"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Additive Decomposition\n",
        "additive_decomposition = seasonal_decompose(data['Sales'], model='additive')\n",
        "additive_decomposition.plot();\n",
        "\n",
        "# Multiplicative Decomposition\n",
        "multiplicative_decomposition = seasonal_decompose(data['Sales'], model='multiplicative')\n",
        "multiplicative_decomposition.plot();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H34PefbR4zj6"
      },
      "source": [
        "üëâ De-seasonalize this time serie using the decomposition of your choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOtKAWDk4zj6"
      },
      "outputs": [],
      "source": [
        "# De-seasonalize the time series using the additive decomposition's residual\n",
        "deseasonalized_data = additive_decomposition.resid.dropna()\n",
        "\n",
        "# You can plot the de-seasonalized data to see the result\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(deseasonalized_data)\n",
        "plt.title('De-seasonalized Monthly Champagne Sales')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales (De-seasonalized)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_B5Gg6S4zj6"
      },
      "source": [
        "‚òùÔ∏è We could fit an ARIMA model on a \"de-seasonalized\" version of this time serie, and then re-compose it back after our forecast. But in this challenge, we will use SARIMA models to _directly_ fit the original time serie!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0cjFvEj4zj6"
      },
      "source": [
        "## 05 - Split the data (train/test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vptvqyHA4zj6"
      },
      "source": [
        "For time series, we cannot randomly sample points to be in the test set. The test set needs to be \"out-of-time\", that is, strictly in the future of the test set. For the purpose of this exercise, we will use data up to 1970 for training and after for the test.\n",
        "\n",
        "üëâ create `df_train` and `df_test`, splitting the DataFrame before 1970 for the training and after (or equal to) for the testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZKT2HTO4zj6"
      },
      "outputs": [],
      "source": [
        "df_train = data[data.index.year < 1970]\n",
        "df_test = data[data.index.year >= 1970]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUmv7saL4zj6"
      },
      "source": [
        "## 06 - SARIMA\n",
        "We will analyse the data using a SARIMA model (Seasonal Auto Regressive Integrated Moving Average).\n",
        "\n",
        "\n",
        "We need to :\n",
        "- find how to stationarize the time serie (I in SARIMA)\n",
        "- find the auto-regressive (AR) part\n",
        "- find the Moving-Average (MA) part\n",
        "- find the seasonality (S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vKtJ_UW4zj6"
      },
      "source": [
        "### Step 1: Check stationarity\n",
        "\n",
        "If a time series has a trend or seasonality component, it must be made stationary before we can use ARIMA to forecast.\n",
        "\n",
        "A quick glance at the plot above should be sufficient to convince you that the time serie is not stationary.\n",
        "\n",
        "Double check the stationarity of `df[\"Sales\"]` using the [`Augmented Dick Fuller test`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html) and especially its p-value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ko3hpH3P4zj6"
      },
      "outputs": [],
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "adf_result = adfuller(data['Sales'])\n",
        "print(f'ADF Statistic: {adf_result[0]}')\n",
        "print(f'p-value: {adf_result[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-eYTOhW4zj6"
      },
      "source": [
        "The p-value should be less than 0.05 to have a 95% confidence in the stationarity.  \n",
        "\n",
        "If the p-value is larger than 0.05, we cannot reject the null hypothesis (null hypothesis = \"the process is not stationary\").\n",
        "\n",
        "Ideally, p is *much* smaller than 0.05.\n",
        "\n",
        "Another way to look for stationarity, is to look at the autocorrelation function (ACF).\n",
        "\n",
        "Plot the ACF of the time series. You should see a large peak at lag 12, indicating strong yearly seasonality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brPw75PF4zj6"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "plot_acf(data['Sales']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sicqo9KU4zj6"
      },
      "source": [
        "### Step 2 - Seasonal differencing (D)\n",
        "\n",
        "We have a strong seasonality with a period of 12 months.\n",
        "\n",
        "Do a **seasonal differencing of lag 12** on the time series, and plot the differenced data and check the ACF plot (+ADF test) again.\n",
        "\n",
        "Don't forget to drop NaN's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYEmciYb4zj7"
      },
      "outputs": [],
      "source": [
        "# Seasonal differencing with lag 12\n",
        "data_diff_12 = data['Sales'].diff(12).dropna()\n",
        "\n",
        "# Plot the differenced data\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data_diff_12)\n",
        "plt.title('Seasonally Differenced Monthly Champagne Sales (Lag 12)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales (Differenced)')\n",
        "plt.show()\n",
        "\n",
        "# ADF test on differenced data\n",
        "adf_result_diff_12 = adfuller(data_diff_12)\n",
        "print(f'ADF Statistic (Seasonal Differencing): {adf_result_diff_12[0]}')\n",
        "print(f'p-value (Seasonal Differencing): {adf_result_diff_12[1]}')\n",
        "\n",
        "# Plot ACF of differenced data\n",
        "plot_acf(data_diff_12);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfHKVlxB4zj7"
      },
      "source": [
        "‚òùÔ∏è One single seasonal differencing seems enough to stationarize the TS.  \n",
        "D=1 (1 seasonal diff) and m=12 (seasonality = 12) for SARIMA(p,d,q, P,D,Q,m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeQ_xQrD4zj7"
      },
      "source": [
        "### Step 3 - Differencing (d)\n",
        "\n",
        "If the time series is still not stationary, it needs to be stationarized through *differencing*. It means that we take the difference between each value and the preceding one (*first difference*).\n",
        "\n",
        "Do we need further differencing after removing the seasonality?\n",
        "\n",
        "Check the ACF and ADF after running an additional first order diff(1) on your diff(12) series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OoNQasa4zj7"
      },
      "outputs": [],
      "source": [
        "# Apply first-order differencing to the seasonally differenced data\n",
        "data_diff_12_1 = data_diff_12.diff(1).dropna()\n",
        "\n",
        "# ADF test on the new differenced data\n",
        "adf_result_diff_12_1 = adfuller(data_diff_12_1)\n",
        "print(f'ADF Statistic (Seasonal and First Differencing): {adf_result_diff_12_1[0]}')\n",
        "print(f'p-value (Seasonal and First Differencing): {adf_result_diff_12_1[1]}')\n",
        "\n",
        "# Plot ACF of the new differenced data\n",
        "plot_acf(data_diff_12_1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRyoZiN44zj7"
      },
      "source": [
        "üëâ Additional differencing actually made both the p-value and the ACF look worse!\n",
        "We can select (d=0) and (D=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkNiRawK4zj7"
      },
      "source": [
        "### Step 4 - Select AR(p,P) and MA(q,Q) terms\n",
        "\n",
        "You will now use the ACF and PACF plots to decide whether to include an AR term(s), MA term(s), or both.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbUtCkG74zj7"
      },
      "source": [
        "- The autocorrelation plot ([`plot_acf`](https://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_acf.html)) applied to your **fully differentiated** series allows you to select MA number (`q`)\n",
        "- MA($\\color{blue}{q}$) = number of lag beyond which the $\\color{blue}{ACF}$ of  $Y^{\\color{green}{(d+D)}}$ cuts off\n",
        "\n",
        "\n",
        "- The partial autocorrelation plot [`plot_pacf`](https://www.statsmodels.org/stable/generated/statsmodels.graphics.tsaplots.plot_pacf.html) applied to your **fully differentiated** series allows you to select AR (`p`)\n",
        "- AR($\\color{red}{p}$) = number of lags beyond which the $\\color{red}{PACF}$ of $Y^{\\color{green}{(d+D)}}$  cuts off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Szn-gYDr4zj7"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "\n",
        "# Plot ACF of the fully differenced data\n",
        "plot_acf(data_diff_12_1);\n",
        "\n",
        "# Plot PACF of the fully differenced data\n",
        "plot_pacf(data_diff_12_1);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIn5LPXB4zj7"
      },
      "source": [
        "‚òùÔ∏è It seems like there are no significant peaks in either ACF or PACF, so most likely p=q=0.  \n",
        "Our first guess for SARIMA is therefore d=0, p=q=0, D=1 and m=12.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwS_pi7e4zj7"
      },
      "source": [
        "How to find values for P and Q? Here, we will let pdarima try values 1 and 0 for P and Q with a grid_search for us.\n",
        "If you really want to find these coefficient by yourself, [Read here ü§Ø ](https://www.datasciencecentral.com/profiles/blogs/tutorial-forecasting-with-seasonal-arima) for some rules of thumb\n",
        "\n",
        "üëâ Run [auto_arima](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html) for the training set. set seasonal=True, m=12 and D=1. Search for p and q, P and Q from 0 to 2. Then, print the summary of the model with model.summary().\n",
        "Use `njobs=-1, trace=True, error_action='ignore', suppress_warnings=True`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy pmdarima\n",
        "!pip install numpy pmdarima\n",
        "from pmdarima import auto_arima\n",
        "!pip install numpy pmdarima"
      ],
      "metadata": {
        "id": "5-h6-OkMgded"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmdarima"
      ],
      "metadata": {
        "id": "ySQtQJndhZTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EP6xadCn4zj7"
      },
      "outputs": [],
      "source": [
        "#!pip uninstall -y numpy pmdarima\n",
        "#!pip install numpy==1.26.4 pmdarima==2.0.4\n",
        "from pmdarima import auto_arima\n",
        "model = auto_arima(df_train['Sales'], seasonal=True, m=12, D=1,\n",
        "                   p=(0, 2), d=0, q=(0, 2),\n",
        "                   P=(0, 2), Q=(0, 2),\n",
        "                   njobs=-1, trace=True, error_action='ignore', suppress_warnings=True)\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmH2HtM34zj7"
      },
      "source": [
        "The best model found is `ARIMA(0,0,0)(0,1,0)[12]` !\n",
        "This means that the Time Serie condidered does not justify adding any \"autoregressive\" nor \"moving average\" modelling beyond simple \"seasonal differencing\".\n",
        "\n",
        "Such \"poor\" model is only likely to predict a repetition of the last seasonal pattern detected, alongwith the prolongation of the mean increasing trend. Let's check it out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXovwba64zj7"
      },
      "source": [
        "üëâ Use `model.predict` with `n_periods` = length of the test set, in order to save the following three predictions\n",
        "- `central`\n",
        "- `upper`\n",
        "- `lower`\n",
        "\n",
        "Save them as Pandas Series, and index them by the same index than your `y_test`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkaTXsq74zj7"
      },
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "forecast_results = model.predict(n_periods=len(df_test), return_conf_int=True)\n",
        "\n",
        "# Extract central, lower, and upper predictions\n",
        "central = pd.Series(forecast_results[0], index=df_test.index)\n",
        "lower = pd.Series(forecast_results[1][:, 0], index=df_test.index)\n",
        "upper = pd.Series(forecast_results[1][:, 1], index=df_test.index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dXpVZyp4zj7"
      },
      "source": [
        "üëá Run the cell below to plot your predictions against reality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pwdj_6EO4zj7"
      },
      "outputs": [],
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(12,5), dpi=100)\n",
        "plt.plot(df_train, label = \"train values\")\n",
        "plt.plot(df_test, label = \"true test values\")\n",
        "plt.plot(central, color='darkgreen',label = \"forecast\")\n",
        "plt.fill_between(lower.index,\n",
        "                 lower,\n",
        "                 upper,\n",
        "                 color='k', alpha=.15)\n",
        "\n",
        "plt.title(\"SARIMA Forecast\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcYGWOeW4zkC"
      },
      "source": [
        "üëâ Try to assign non-null values to  `p`,`d`,`P`,`D` on your SARIMA and re-plot this chart to compare effects!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jJ0fIvV4zkC"
      },
      "source": [
        "#  Machine Learning Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbv5-I5R4zkC"
      },
      "source": [
        "### Model with forecast horizon = 1 month only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VeB87QM4zkC"
      },
      "source": [
        "We will now fit a non linear model such as a random forest. The idea is to predict a value from the last ones.\n",
        "\n",
        "üëâ Create new columns in `df` that are shifted version of `df['Sales']`. Do it with a shift from 1 to 12."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcY2oM5x4zkC"
      },
      "outputs": [],
      "source": [
        "# Add lagged values\n",
        "\n",
        "# Drop nan\n",
        "\n",
        "\n",
        "# create df_train and df_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lCE6FZ54zkC"
      },
      "outputs": [],
      "source": [
        "# Keep track of test_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iajLybUM4zkC"
      },
      "outputs": [],
      "source": [
        "# Create X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bjLX7AC4zkC"
      },
      "source": [
        "üëâ Instanciate (don't train) a `random forest` algorithm to predict Sales based in your shifted features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV8aCmiI4zkD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMlazbH4zkD"
      },
      "source": [
        "üëâ You can use the following function to test it. It takes the true y values (`data` which should be a Numpy array), the indexes of the test samples (`test_indexes`), the predictor (`predictor`: your random forest algorithm) and the shifted columns (`full_X` which should be a Numpy array)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOzn8QVw4zkD"
      },
      "outputs": [],
      "source": [
        "# For each predictor, design a method to evaluate its performance on the test set:\n",
        "from sklearn.metrics import r2_score\n",
        "def evaluate_performance_month_prediction(y_true, test_indexes, predictor, full_X):\n",
        "    predictions = []\n",
        "    ground_truth = []\n",
        "    i = 0\n",
        "    for _indx in test_indexes:\n",
        "        train_data = y_true[:_indx]\n",
        "        current_ground_truth = y_true[_indx]\n",
        "        current_ground_truth_features = full_X[_indx,:]\n",
        "        train_features = full_X[:_indx]\n",
        "        # train the model on all datapoint until (t-1) in order to predict (t)\n",
        "        predictor.fit(train_features, train_data)\n",
        "        prediction = predictor.predict(current_ground_truth_features.reshape(1,-1))[0]\n",
        "        predictions.append(prediction)\n",
        "        ground_truth.append(current_ground_truth)\n",
        "        i += 1\n",
        "    mape = np.mean(np.abs(np.array(predictions) - np.array(ground_truth))/np.abs(np.array(ground_truth)))\n",
        "    return ground_truth, predictions, mape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "BkL1GnHp4zkD"
      },
      "outputs": [],
      "source": [
        "results = evaluate_performance_month_prediction(y.values, test_indexes=test_indexes,\n",
        "                                                         predictor=random_forest, full_X = X.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4Juhz9I4zkD"
      },
      "outputs": [],
      "source": [
        "print('Mean Absolute Prediction Error MAPE')\n",
        "results[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgww01UR4zkD"
      },
      "outputs": [],
      "source": [
        "plt.plot(results[0], c='black', label='test set')\n",
        "plt.plot(results[1], c='orange', label='forecast (horizon = 1)')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0QCeOIm4zkD"
      },
      "source": [
        "To improve our model, we can do some feature engineering. You will add moving averages to the data used to train the random forest.\n",
        "\n",
        "Try to create 3 new columnns in `df`: one which is the rolling average of `df[Sales]` with a window of 12, one with a window of 3, and one with a window of 2. This will have the effect to isolate the trend and allow the algorithm to learn it. Also, plot these data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A7YzMcs4zkD"
      },
      "outputs": [],
      "source": [
        "# Extract the trend using a well chosen moving average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfx5mf7j4zkD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHLYLMLZ4zkD"
      },
      "source": [
        "You can also add a more smoothing predictor using the exponential moving average (hint: method `.ewm` with `halflife` of 2, 3 and 12), that statistically optimizes an AR process. Plot also the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy-zfrt64zkD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM2Ohj8Z4zkD"
      },
      "source": [
        "You should see a better MAPE!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BN8bAlaw4zkD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbBBjx3u4zkD"
      },
      "source": [
        "Try to look at the importance of each feature. What do you find?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2ApmUgH4zkD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9I4ffXa4zkD"
      },
      "source": [
        "### Out-of-sample forecast models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xwHLRxS4zkD"
      },
      "source": [
        "Let's now try to beat SARIMA in the \"out-of-sample\" prediction.\n",
        "\n",
        "üëâ Train _one model per horizon of forecast_ (from `1` month horizon to `len(y_test)` months)  \n",
        "üëâ Evaluate MAPE and compare with SARIMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxChcCj04zkD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}